{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598235fe",
   "metadata": {},
   "source": [
    "###### 1.Identifying missing values: \n",
    "-To discover information in our data we can use info() and duplicated() methods, and also isnull() or isna():\n",
    "\n",
    "-info(): This method provides a concise summary of the DataFrame, including the count of non-null values for each column. Columns with missing values will have a count less than the total number of rows.\n",
    "\n",
    "-isnull() or isna() methods: These methods return a DataFrame of the same shape as the original, where each cell is True if the corresponding element is NaN (missing), and False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51865dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 169 entries, 0 to 168\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Duration  169 non-null    int64  \n",
      " 1   Pulse     169 non-null    int64  \n",
      " 2   Maxpulse  169 non-null    int64  \n",
      " 3   Calories  164 non-null    float64\n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 5.4 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "print(df.info()) # Summary of non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dc45e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Duration  Pulse  Maxpulse  Calories\n",
      "0       False  False     False     False\n",
      "1       False  False     False     False\n",
      "2       False  False     False     False\n",
      "3       False  False     False     False\n",
      "4       False  False     False     False\n",
      "..        ...    ...       ...       ...\n",
      "164     False  False     False     False\n",
      "165     False  False     False     False\n",
      "166     False  False     False     False\n",
      "167     False  False     False     False\n",
      "168     False  False     False     False\n",
      "\n",
      "[169 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df=pd.read_csv('data.csv')\n",
    "print(df.isnull())  # Boolean mask indicating missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f581e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      False\n",
      "1      False\n",
      "2      False\n",
      "3      False\n",
      "4      False\n",
      "       ...  \n",
      "164    False\n",
      "165    False\n",
      "166    False\n",
      "167    False\n",
      "168    False\n",
      "Length: 169, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(df.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdffe90",
   "metadata": {},
   "source": [
    "###### Handling Missing Values:\n",
    "-You may decide to use the dropna() method to remove rows or columns containing missing values if the amount of missing values is minimal in relation to the dataset's size and has no apparent effect on the analysis.\n",
    "\n",
    "-Replace an appropriate estimate for any missing values. Typical approaches include utilizing the fillna() function to fill in missing data with the mean, median, mode, or a constant value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "354f1465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Duration  Pulse  Maxpulse  Calories\n",
      "0          60    110       130     409.1\n",
      "1          60    117       145     479.0\n",
      "2          60    103       135     340.0\n",
      "3          45    109       175     282.4\n",
      "4          45    117       148     406.0\n",
      "..        ...    ...       ...       ...\n",
      "164        60    105       140     290.8\n",
      "165        60    110       145     300.0\n",
      "166        60    115       145     310.2\n",
      "167        75    120       150     320.4\n",
      "168        75    125       150     330.4\n",
      "\n",
      "[169 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.dropna())  # Drop rows with any missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af3720ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Duration  Pulse  Maxpulse  Calories\n",
      "0          60    110       130     409.1\n",
      "1          60    117       145     479.0\n",
      "2          60    103       135     340.0\n",
      "3          45    109       175     282.4\n",
      "4          45    117       148     406.0\n",
      "5          60    102       127     300.0\n",
      "6          60    110       136     374.0\n",
      "7          45    104       134     253.3\n",
      "8          30    109       133     195.1\n",
      "9          60     98       124     269.0\n",
      "10         60    103       147     329.3\n",
      "11         60    100       120     250.7\n",
      "12         60    106       128     345.3\n",
      "13         60    104       132     379.3\n",
      "14         60     98       123     275.0\n",
      "15         60     98       120     215.2\n",
      "16         60    100       120     300.0\n",
      "17         45     90       112      50.0\n",
      "18         60    103       123     323.0\n",
      "19         45     97       125     243.0\n",
      "20         60    108       131     364.2\n",
      "21         45    100       119     282.0\n",
      "22         60    130       101     300.0\n",
      "23         45    105       132     246.0\n",
      "24         60    102       126     334.5\n",
      "25         60    100       120     250.0\n",
      "26         60     92       118     241.0\n",
      "27         60    103       132      50.0\n",
      "28         60    100       132     280.0\n",
      "29         60    102       129     380.3\n",
      "30         60     92       115     243.0\n",
      "31         45     90       112     180.1\n",
      "32         60    101       124     299.0\n",
      "33         60     93       113     223.0\n",
      "34         60    107       136     361.0\n",
      "35         60    114       140     415.0\n",
      "36         60    102       127     300.0\n",
      "37         60    100       120     300.0\n",
      "38         60    100       120     300.0\n",
      "39         45    104       129     266.0\n",
      "40         45     90       112     180.1\n",
      "41         60     98       126     286.0\n",
      "42         60    100       122     329.4\n",
      "43         60    111       138     400.0\n",
      "44         60    111       131     397.0\n",
      "45         60     99       119     273.0\n",
      "46         60    109       153     387.6\n",
      "47         45    111       136     300.0\n",
      "48         45    108       129     298.0\n",
      "49         60    111       139     397.6\n",
      "50         60    107       136     380.2\n",
      "51         80    123       146     643.1\n",
      "52         60    106       130     263.0\n",
      "53         60    118       151     486.0\n",
      "54         30    136       175     238.0\n",
      "55         60    121       146     450.7\n",
      "56         60    118       121     413.0\n",
      "57         45    115       144     305.0\n",
      "58         20    153       172     226.4\n",
      "59         45    123       152     321.0\n",
      "60        210    108       160    1376.0\n",
      "61        160    110       137    1034.4\n",
      "62        160    109       135     853.0\n",
      "63         45    118       141     341.0\n",
      "64         20    110       130     131.4\n",
      "65        180     90       130     800.4\n",
      "66        150    105       135     873.4\n",
      "67        150    107       130     816.0\n",
      "68         20    106       136     110.4\n",
      "69        300    108       143    1500.2\n",
      "70        150     97       129    1115.0\n",
      "71         60    109       153     387.6\n",
      "72         90    100       127     700.0\n",
      "73        150     97       127     953.2\n",
      "74         45    114       146     304.0\n",
      "75         90     98       125     563.2\n",
      "76         45    105       134     251.0\n",
      "77         45    110       141     300.0\n",
      "78        120    100       130     500.4\n",
      "79        270    100       131    1729.0\n",
      "80         30    159       182     319.2\n",
      "81         45    149       169     344.0\n",
      "82         30    103       139     151.1\n",
      "83        120    100       130     500.0\n",
      "84         45    100       120     225.3\n",
      "85         30    151       170     300.0\n",
      "86         45    102       136     234.0\n",
      "87        120    100       157    1000.1\n",
      "88         45    129       103     242.0\n",
      "89         20     83       107      50.3\n",
      "90        180    101       127     600.1\n",
      "91         45    107       137      50.0\n",
      "92         30     90       107     105.3\n",
      "93         15     80       100      50.5\n",
      "94         20    150       171     127.4\n",
      "95         20    151       168     229.4\n",
      "96         30     95       128     128.2\n",
      "97         25    152       168     244.2\n",
      "98         30    109       131     188.2\n",
      "99         90     93       124     604.1\n",
      "100        20     95       112      77.7\n",
      "101        90     90       110     500.0\n",
      "102        90     90       100     500.0\n",
      "103        90     90       100     500.4\n",
      "104        30     92       108      92.7\n",
      "105        30     93       128     124.0\n",
      "106       180     90       120     800.3\n",
      "107        30     90       120      86.2\n",
      "108        90     90       120     500.3\n",
      "109       210    137       184    1860.4\n",
      "110        60    102       124     325.2\n",
      "111        45    107       124     275.0\n",
      "112        15    124       139     124.2\n",
      "113        45    100       120     225.3\n",
      "114        60    108       131     367.6\n",
      "115        60    108       151     351.7\n",
      "116        60    116       141     443.0\n",
      "117        60     97       122     277.4\n",
      "118        60    105       125      50.0\n",
      "119        60    103       124     332.7\n",
      "120        30    112       137     193.9\n",
      "121        45    100       120     100.7\n",
      "122        60    119       169     336.7\n",
      "123        60    107       127     344.9\n",
      "124        60    111       151     368.5\n",
      "125        60     98       122     271.0\n",
      "126        60     97       124     275.3\n",
      "127        60    109       127     382.0\n",
      "128        90     99       125     466.4\n",
      "129        60    114       151     384.0\n",
      "130        60    104       134     342.5\n",
      "131        60    107       138     357.5\n",
      "132        60    103       133     335.0\n",
      "133        60    106       132     327.5\n",
      "134        60    103       136     339.0\n",
      "135        20    136       156     189.0\n",
      "136        45    117       143     317.7\n",
      "137        45    115       137     318.0\n",
      "138        45    113       138     308.0\n",
      "139        20    141       162     222.4\n",
      "140        60    108       135     390.0\n",
      "141        60     97       127      50.0\n",
      "142        45    100       120     250.4\n",
      "143        45    122       149     335.4\n",
      "144        60    136       170     470.2\n",
      "145        45    106       126     270.8\n",
      "146        60    107       136     400.0\n",
      "147        60    112       146     361.9\n",
      "148        30    103       127     185.0\n",
      "149        60    110       150     409.4\n",
      "150        60    106       134     343.0\n",
      "151        60    109       129     353.2\n",
      "152        60    109       138     374.0\n",
      "153        30    150       167     275.8\n",
      "154        60    105       128     328.0\n",
      "155        60    111       151     368.5\n",
      "156        60     97       131     270.4\n",
      "157        60    100       120     270.4\n",
      "158        60    114       150     382.8\n",
      "159        30     80       120     240.9\n",
      "160        30     85       120     250.4\n",
      "161        45     90       130     260.4\n",
      "162        45     95       130     270.0\n",
      "163        45    100       140     280.9\n",
      "164        60    105       140     290.8\n",
      "165        60    110       145     300.0\n",
      "166        60    115       145     310.2\n",
      "167        75    120       150     320.4\n",
      "168        75    125       150     330.4\n"
     ]
    }
   ],
   "source": [
    "df.fillna(50, inplace=True)\n",
    "print(df.to_string())  # Fill missing values with number 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c38405",
   "metadata": {},
   "source": [
    "###### Imputation involves substituting values for lacking data. The replaced values are generally approximated using different statistical techniques or generated from the known portion of the data. Several factors make imputation helpful when handling missing data:\n",
    " \n",
    "-Preservation of Data Integrity\n",
    "\n",
    "-Maintaining Sample Size\n",
    "\n",
    "-Improved Analysis Accuracy\n",
    "\n",
    "-Handling Practical Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e13244b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Duration  Pulse  Maxpulse  Calories\n",
      "0          60    110       130     409.1\n",
      "1          60    117       145     479.0\n",
      "2          60    103       135     340.0\n",
      "3          45    109       175     282.4\n",
      "4          45    117       148     406.0\n",
      "5          60    102       127     300.0\n",
      "6          60    110       136     374.0\n",
      "7          45    104       134     253.3\n",
      "8          30    109       133     195.1\n",
      "9          60     98       124     269.0\n",
      "10         60    103       147     329.3\n",
      "11         60    100       120     250.7\n",
      "12         60    106       128     345.3\n",
      "13         60    104       132     379.3\n",
      "14         60     98       123     275.0\n",
      "15         60     98       120     215.2\n",
      "16         60    100       120     300.0\n",
      "17         45     90       112      50.0\n",
      "18         60    103       123     323.0\n",
      "19         45     97       125     243.0\n",
      "20         60    108       131     364.2\n",
      "21         45    100       119     282.0\n",
      "22         60    130       101     300.0\n",
      "23         45    105       132     246.0\n",
      "24         60    102       126     334.5\n",
      "25         60    100       120     250.0\n",
      "26         60     92       118     241.0\n",
      "27         60    103       132      50.0\n",
      "28         60    100       132     280.0\n",
      "29         60    102       129     380.3\n",
      "30         60     92       115     243.0\n",
      "31         45     90       112     180.1\n",
      "32         60    101       124     299.0\n",
      "33         60     93       113     223.0\n",
      "34         60    107       136     361.0\n",
      "35         60    114       140     415.0\n",
      "36         60    102       127     300.0\n",
      "37         60    100       120     300.0\n",
      "38         60    100       120     300.0\n",
      "39         45    104       129     266.0\n",
      "40         45     90       112     180.1\n",
      "41         60     98       126     286.0\n",
      "42         60    100       122     329.4\n",
      "43         60    111       138     400.0\n",
      "44         60    111       131     397.0\n",
      "45         60     99       119     273.0\n",
      "46         60    109       153     387.6\n",
      "47         45    111       136     300.0\n",
      "48         45    108       129     298.0\n",
      "49         60    111       139     397.6\n",
      "50         60    107       136     380.2\n",
      "51         80    123       146     643.1\n",
      "52         60    106       130     263.0\n",
      "53         60    118       151     486.0\n",
      "54         30    136       175     238.0\n",
      "55         60    121       146     450.7\n",
      "56         60    118       121     413.0\n",
      "57         45    115       144     305.0\n",
      "58         20    153       172     226.4\n",
      "59         45    123       152     321.0\n",
      "60        210    108       160    1376.0\n",
      "61        160    110       137    1034.4\n",
      "62        160    109       135     853.0\n",
      "63         45    118       141     341.0\n",
      "64         20    110       130     131.4\n",
      "65        180     90       130     800.4\n",
      "66        150    105       135     873.4\n",
      "67        150    107       130     816.0\n",
      "68         20    106       136     110.4\n",
      "69        300    108       143    1500.2\n",
      "70        150     97       129    1115.0\n",
      "71         60    109       153     387.6\n",
      "72         90    100       127     700.0\n",
      "73        150     97       127     953.2\n",
      "74         45    114       146     304.0\n",
      "75         90     98       125     563.2\n",
      "76         45    105       134     251.0\n",
      "77         45    110       141     300.0\n",
      "78        120    100       130     500.4\n",
      "79        270    100       131    1729.0\n",
      "80         30    159       182     319.2\n",
      "81         45    149       169     344.0\n",
      "82         30    103       139     151.1\n",
      "83        120    100       130     500.0\n",
      "84         45    100       120     225.3\n",
      "85         30    151       170     300.0\n",
      "86         45    102       136     234.0\n",
      "87        120    100       157    1000.1\n",
      "88         45    129       103     242.0\n",
      "89         20     83       107      50.3\n",
      "90        180    101       127     600.1\n",
      "91         45    107       137      50.0\n",
      "92         30     90       107     105.3\n",
      "93         15     80       100      50.5\n",
      "94         20    150       171     127.4\n",
      "95         20    151       168     229.4\n",
      "96         30     95       128     128.2\n",
      "97         25    152       168     244.2\n",
      "98         30    109       131     188.2\n",
      "99         90     93       124     604.1\n",
      "100        20     95       112      77.7\n",
      "101        90     90       110     500.0\n",
      "102        90     90       100     500.0\n",
      "103        90     90       100     500.4\n",
      "104        30     92       108      92.7\n",
      "105        30     93       128     124.0\n",
      "106       180     90       120     800.3\n",
      "107        30     90       120      86.2\n",
      "108        90     90       120     500.3\n",
      "109       210    137       184    1860.4\n",
      "110        60    102       124     325.2\n",
      "111        45    107       124     275.0\n",
      "112        15    124       139     124.2\n",
      "113        45    100       120     225.3\n",
      "114        60    108       131     367.6\n",
      "115        60    108       151     351.7\n",
      "116        60    116       141     443.0\n",
      "117        60     97       122     277.4\n",
      "118        60    105       125      50.0\n",
      "119        60    103       124     332.7\n",
      "120        30    112       137     193.9\n",
      "121        45    100       120     100.7\n",
      "122        60    119       169     336.7\n",
      "123        60    107       127     344.9\n",
      "124        60    111       151     368.5\n",
      "125        60     98       122     271.0\n",
      "126        60     97       124     275.3\n",
      "127        60    109       127     382.0\n",
      "128        90     99       125     466.4\n",
      "129        60    114       151     384.0\n",
      "130        60    104       134     342.5\n",
      "131        60    107       138     357.5\n",
      "132        60    103       133     335.0\n",
      "133        60    106       132     327.5\n",
      "134        60    103       136     339.0\n",
      "135        20    136       156     189.0\n",
      "136        45    117       143     317.7\n",
      "137        45    115       137     318.0\n",
      "138        45    113       138     308.0\n",
      "139        20    141       162     222.4\n",
      "140        60    108       135     390.0\n",
      "141        60     97       127      50.0\n",
      "142        45    100       120     250.4\n",
      "143        45    122       149     335.4\n",
      "144        60    136       170     470.2\n",
      "145        45    106       126     270.8\n",
      "146        60    107       136     400.0\n",
      "147        60    112       146     361.9\n",
      "148        30    103       127     185.0\n",
      "149        60    110       150     409.4\n",
      "150        60    106       134     343.0\n",
      "151        60    109       129     353.2\n",
      "152        60    109       138     374.0\n",
      "153        30    150       167     275.8\n",
      "154        60    105       128     328.0\n",
      "155        60    111       151     368.5\n",
      "156        60     97       131     270.4\n",
      "157        60    100       120     270.4\n",
      "158        60    114       150     382.8\n",
      "159        30     80       120     240.9\n",
      "160        30     85       120     250.4\n",
      "161        45     90       130     260.4\n",
      "162        45     95       130     270.0\n",
      "163        45    100       140     280.9\n",
      "164        60    105       140     290.8\n",
      "165        60    110       145     300.0\n",
      "166        60    115       145     310.2\n",
      "167        75    120       150     320.4\n",
      "168        75    125       150     330.4\n"
     ]
    }
   ],
   "source": [
    "a = df.fillna(df.mean())\n",
    "print(a.to_string()) # Fill missing values with mean(imputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b25fb8",
   "metadata": {},
   "source": [
    "###### 2.  There are three methods of encoding:\n",
    "\n",
    "Ordinal Encoding: This approach works well with categorical variables that have an inherent ordering. Based on their ordinal relationship, categories can be manually given numerical values.\n",
    "\n",
    "One-Hot Encoding: When there is no ordinal link between the categories, one-hot encoding is employed. For every category, it generates binary columns that show whether the category is present or absent for every observation.\n",
    "\n",
    "Label Encoding: Using this technique, every category is given a distinct integer. When there are many categories in the categorical variable and one-hot encoding would produce a high-dimensional sparse matrix, it is appropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56b18459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Score\n",
      "0     Low\n",
      "1     Low\n",
      "2  Medium\n",
      "3  Medium\n",
      "4    High\n",
      "5     Low\n",
      "6  Medium\n",
      "7    High\n",
      "8     Low\n"
     ]
    }
   ],
   "source": [
    "# Sample of data\n",
    "df = pd.DataFrame({\"Score\": [\"Low\", \"Low\", \"Medium\", \"Medium\", \"High\", \"Low\", \"Medium\",\"High\", \"Low\"]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a44c491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Score  Scale\n",
      "0     Low      1\n",
      "1     Low      1\n",
      "2  Medium      2\n",
      "3  Medium      2\n",
      "4    High      3\n",
      "5     Low      1\n",
      "6  Medium      2\n",
      "7    High      3\n",
      "8     Low      1\n"
     ]
    }
   ],
   "source": [
    "scale_mapper = {\"Low\":1, \"Medium\":2, \"High\":3}\n",
    "df[\"Scale\"] = df[\"Score\"].replace(scale_mapper)\n",
    "print(df) # Ordinal encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28919f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category_A  Category_B  Category_C\n",
      "0        True       False       False\n",
      "1       False        True       False\n",
      "2        True       False       False\n",
      "3       False       False        True\n",
      "4       False        True       False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {'Category': ['A', 'B', 'A', 'C', 'B']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=['Category'])\n",
    "print(df_encoded) # One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c26229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gender    Name\n",
      "0      F   Cindy\n",
      "1      M  Johnny\n",
      "2      F    Sara\n",
      "3      M  Victor\n",
      "4      F  Martha\n",
      "5      M     Max\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing   \n",
    "obj = preprocessing.LabelEncoder()  \n",
    "\n",
    "\n",
    "import pandas as pd   \n",
    "my_data = {  \n",
    "    \"Gender\" : ['F', 'M', 'F','M', 'F','M'],  \n",
    "    \"Name\" : ['Cindy','Johnny','Sara', 'Victor', 'Martha','Max']  \n",
    "        }  \n",
    "blk = pd.DataFrame(my_data)  \n",
    "print(blk)   # before using label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef7ae57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "   Gender    Name\n",
      "0       0   Cindy\n",
      "1       1  Johnny\n",
      "2       0    Sara\n",
      "3       1  Victor\n",
      "4       0  Martha\n",
      "5       1     Max\n"
     ]
    }
   ],
   "source": [
    "my_label = preprocessing.LabelEncoder()   \n",
    "   \n",
    "blk[ 'Gender' ]= my_label.fit_transform(blk[ 'Gender' ])   \n",
    "print(blk[ 'Gender' ].unique())  \n",
    "print( blk )  # after lable encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be3177",
   "metadata": {},
   "source": [
    "- One hot encoding is a technique that we use to represent categorical variables as numerical values in a machine learning model.\n",
    "In One Hot Encoding, the categorical parameters will prepare separate columns for both Male and Female labels. So, wherever there is a Male, the value will be 1 in the Male column and 0 in the Female column, and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a38e6",
   "metadata": {},
   "source": [
    "###### 3. Removing Duplicates Questions:\n",
    "\n",
    "- We can identify duplicates by using duplicated method().\n",
    "- And directly remove by using drop_duplicates() method. This method returns a DataFrame with duplicate rows removed. By default, it keeps the first occurrence of each duplicated row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65e394c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows:\n",
      "   A  B  C\n",
      "4  2  b  y\n",
      "\n",
      "DataFrame without Duplicates:\n",
      "   A  B  C\n",
      "0  1  a  x\n",
      "1  2  b  y\n",
      "2  3  c  z\n",
      "3  4  d  x\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with duplicate rows\n",
    "data = {'A': [1, 2, 3, 4, 2],\n",
    "        'B': ['a', 'b', 'c', 'd', 'b'],\n",
    "        'C': ['x', 'y', 'z', 'x', 'y']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Identifying duplicate rows\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "print(\"Duplicate Rows:\")\n",
    "print(duplicate_rows)\n",
    "\n",
    "# Removing duplicate rows\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(\"\\nDataFrame without Duplicates:\")\n",
    "print(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bded2",
   "metadata": {},
   "source": [
    "-The differene between duplicated() and drop_duplicates():\n",
    "\n",
    "duplicated() is used for identifying duplicated rows and uses False and True for identification. \n",
    "\n",
    "drop_duplicates() is used for removing duplicated rows. It returns a new DataFrame with duplicate rows removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a3a3a",
   "metadata": {},
   "source": [
    "###### 4. Data Scaling and Normalization Questions:\n",
    "\n",
    "Machine learning models can perform much better when features are scaled. Because the various feature scales have no effect on the algorithms, scaling the features facilitates the search for the best answer.\n",
    "\n",
    "-It facilitates faster algorithm convergence. This is particularly true for gradient descent-based algorithms, as they may optimize the cost function more quickly.\n",
    "\n",
    "-Feature scaling also helps with interpretability since it brings the magnitude of all features on the same scale. \n",
    "\n",
    "-The robustness to outliers can also be increased by feature scaling, however this does not mean that feature scaling should be used as a strategy for dealing with outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7715d94d",
   "metadata": {},
   "source": [
    "###### Min-Max\n",
    "-One of the most used techniques for normalizing data is min-max normalization. All features have their minimum value converted to a zero, their maximum value converted to a one, and all other values converted to a decimal between 0 and 1.\n",
    "\n",
    "-This transformation linearly scales each feature, preserving the relative differences between values.\n",
    "\n",
    "-Min-max scaling is sensitive to outliers since it scales the data based on the range of values, which can be influenced by outliers.\n",
    "\n",
    "- Z-score\n",
    "\n",
    "-Z-score normalization is a strategy of normalizing data that avoids this outlier issue.\n",
    "\n",
    "-This transformation centers the data around the mean and scales it based on the standard deviation.\n",
    "\n",
    "-Z-score normalization is less sensitive to outliers compared to min-max scaling because it uses the mean and standard deviation, which are less influenced by outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794926af",
   "metadata": {},
   "source": [
    "###### Outliers\n",
    "\n",
    "A stray data point that differs greatly from the majority is called an outlier, and it can affect machine learning models' performance in both good and negative ways.\n",
    "\n",
    "-1. Model Performance:\n",
    "Outliers can ruin patterns, so it will give inaccurate data. \n",
    "\n",
    "-2. Overfitting:\n",
    "Because outliers capture noise in the data instead of actual patterns, they can cause overfitting in complex models. When trained on datasets including outliers, models may get attuned to the noise, leading to subpar generalization on unobserved data.\n",
    "\n",
    "-3. Robustness:\n",
    "Positively, some algorithms for machine learning are less vulnerable to outliers. Random forests and decision trees, for example, may tolerate outliers well and continue to function well in their presence.\n",
    "\n",
    "\n",
    "-5. Preprocessing Techniques:\n",
    "Robust preprocessing procedures are essential to reduce the influence of outliers. These include using robust estimators to impute missing values, altering skewed features to lessen the impact of outliers, and capping or extending outliers to predetermined ranges.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15267d7a",
   "metadata": {},
   "source": [
    "###### Detecting outliers:\n",
    "\n",
    "- Z-score:\n",
    "z = (x - mean) / std\n",
    "-x-data point\n",
    "-mean-mean of data set\n",
    "-std - standart deviation \n",
    "\n",
    "To identify outliers using the z-score, we can set a threshold value, say 3. Any data point with a z-score greater than 3 or less than -3 can be considered an outlier. We can use the scipy library in Python to calculate the z-score and identify outliers.\n",
    "\n",
    "The interquartile range (IQR) is a measure of the spread of the middle 50% of the data. The IQR can be calculated as the difference between the 75th percentile and the 25th percentile of the dataset. Any data point outside the range of 1.5 times the IQR below the 25th percentile or above the 75th percentile can be considered an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f60d5803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers:\n",
      "   Age\n",
      "2   44\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "# Load the Iris dataset\n",
    "df = pd.DataFrame({\"Age\": [17, 16, 44, 16, 15, 18, 16, 17, 18]})\n",
    "\n",
    " \n",
    "# Extract the column of interest for outlier detection (e.g., age)\n",
    "column_name = \"Age\"\n",
    "data = df[column_name]\n",
    " \n",
    "# Calculate the Z-scores\n",
    "z_scores = (data - data.mean()) / data.std()\n",
    " \n",
    "# Define a threshold for identifying outliers (e.g., Z-score threshold of 2)\n",
    "threshold = 2\n",
    " \n",
    "# Identify the outliers\n",
    "outliers = df[abs(z_scores) > threshold]\n",
    " \n",
    "# Print the outliers\n",
    "print(\"Outliers:\")\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b2feaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age\n",
      "2   44\n"
     ]
    }
   ],
   "source": [
    "# calculate IQR for column Height\n",
    "Q1 = df['Age'].quantile(0.25)\n",
    "Q3 = df['Age'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# identify outliers\n",
    "threshold = 1.5\n",
    "outliers = df[(df['Age'] < Q1 - threshold * IQR) | (df['Age'] > Q3 + threshold * IQR)]\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384094aa",
   "metadata": {},
   "source": [
    "###### Handling Outliers\n",
    "\n",
    "\n",
    "- Trimming:\n",
    "\n",
    "Remove extreme values from the dataset based on a predefined threshold.\n",
    "It involves discarding data points that exceed a certain percentile or fall outside a specified range.\n",
    "\n",
    "- Winsorization:\n",
    "\n",
    "Winsorization replaces extreme values with the values of the nearest non-outlier data points.\n",
    "It helps reduce the impact of outliers without removing them entirely.\n",
    "\n",
    "- Robust Scaling:\n",
    "\n",
    "Use robust scaling techniques such as RobustScaler from scikit-learn to scale the data while mitigating the influence of outliers.\n",
    "Robust scaling scales the data based on robust statistics like median and interquartile range (IQR).\n",
    "\n",
    "- Imputation:\n",
    "\n",
    "Replace outlier values with more representative values using imputation techniques such as median or mean imputation.\n",
    "Imputation can help retain data integrity while mitigating the influence of outliers on the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5235baea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
